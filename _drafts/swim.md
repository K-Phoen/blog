---
layout: post
title: "Clusters and membership: discovering the SWIM protocol"
description: >
    Let's talk about SWIM!
---

Recently, I have been playing with distributed systems in Go. I am toying around
with a distributed key-value store. Because why not?

This store — again, that I am building just for the sake of learning — will be
able to operate as a single instance on a server as well as inside a cluster.
And instead of defining statically through a configuration file which nodes are
part for the cluster, I want to be able to add or remove nodes dynamically.

Each node in the cluster must be able to answer a seemingly simple question:
« *Who are my (active) peers?* »
Or expressed in a more fancy manner, how can the cluster **detect nodes addition,
nodes removal, nodes failures and propagate the information** across the whole
cluster?

## Heartbeat

Traditionally, distributed peer-to-peer applications rely on *heart-beat style*
protocols to detect node failures. When a failure is detected, it is usually
communicated to the cluster using some kind of broadcast or multicast subsystem.

We could summarize the process like this:
 * every node sends an "heartbeat signal" every <code>T</code> interval of time to all
    other nodes ;
 * if no heartbeat from a node <code>N</code> is received after <code>T*limit</code>,
    the non-responsive node is considered dead.

<figure>
    <img src="/img/heartbeat.svg" />
    <figcaption>Heartbeat sent to a 4-nodes cluster</figcaption>
</figure>

The regularly sent heartbeats solve the failure detection problem and we can
also use the broadcast communication channel to transmit information related to
which nodes are in the cluster. So this protocol solves both the failure
detection problem and the *member-list* one.

But when we look more closely at what happens in the network, we notice that the
more the cluster grows, the more messages are sent.
For instance, a four-nodes cluster will send twelve heartbeats every
<code>T</code> interval, a five-nodes one will send twenty, a six-nodes one will
send thirty, … **The number of signals grows quadratically with heartbeat-style
protocols** and this means that a too large cluster could take down the whole
network.

## Protocol evaluation criteria

After noticing this first limitation in the heartbeat-based method, I
realized that I actually needed a way to objectively evaluate protocols.

Luckily, a paper called « *On scalable and efficient distributed failure detectors* »
published in 2001 by I. Gupta, T. D. Chandra, and G. S. Goldszmidt addresses this
topic.

According to them, the key characteristics of the efficiency and scalability of
distributed failure detector protocols are:

* **Strong Completeness**: crash-failure of any group member is detected by all non-faulty members ;
* **Speed of failure detection**: the time interval between a member failure and its detection by some non-faulty group member ;
* **Accuracy**: the rate of false positives of failure detection (note: [100%
  accuracy is impossible on asynchronous networks](http://www.ecommons.cornell.edu/bitstream/1813/7192/1/95-1535.pdf)) ;
* **Network Message Load**, in bytes per second generated by the protocol.

## Evaluating heartbeat

In the light of our evaluation criteria, what can we say about heartbeating?

* **Strong Completeness**: every nodes receives heartbeats, so a node failure
    will be detected independently by every other node in the cluster ;
* **Speed of failure detection**: this is tunable and is often set to a value
    looking like **<code>T * limit</code>** which means that a failure will be
    detected after <code>limit</code> intervals <code>T</code> of time ;
* **Network Message Load**: **<code>O(n²)</code>**, where <code>n</code> is the
    number of nodes in the cluster.
* **Accuracy**: there is a trade-off to be made here. No network being perfect,
    there will always be a portion of the packets that will be lost. So if the
    <code>limit</code> value is set too low (to 1, for instance), a node might
    be considered dead just because the heartbeat was dropped by the network.
    On the other hand, if <code>limit</code> is set to a value that is too high,
    the speed of failure detection will increase accordingly.
    The **trade-off** is therefore **between speed and accuracy**. With a
    reliable-enough network, we can safely assume that a low <code>limit</code>
    will suffice and hence we consider heartbeating as having a "**reasonably
    high**" accuracy.

Back in 2002, a group of researchers from Cornell University (the same three as
previously) devised a new protocol to address the network load imposed by
traditional heart-beating protocols. They called their new protocol SWIM.

## Enters *SWIM*

The new idea introduced in *SWIM* — or **S**calable **W**eakly-consistent
**I**nfection-style Process Group **M**embership Protocol — is the separation of
the failure detection and membership update dissemination functionalities.

### Failure detection component

As the name suggests, the role of this component is to detect potential failures
of nodes in the cluster.

Like in heartbeating protocols, each node has a list of the nodes that are in
the cluster. But instead of brutally pinging all of them every <code>T</code>
interval of time, only one of them will be checked.

On this schema, we see a node **randomly choosing** another one of its peers to
check if it is still alive. It sends a <code>PING</code> message and expects an
<code>ACK</code> in response, indicating that the target node is healthy.

<figure>
    <img src="/img/swim_ping_ack.svg" />
    <figcaption>SWIM: node successfully pinged</figcaption>
</figure>

**If no <code>ACK</code>** from the node *C* **is received** by *A* in the pre-specified
timeout period (determined by the message round-trip time, which is chosen
smaller than the protocol period), then **an indirect check is performed**.

*A* will select <code>k</code> members at random from its list of nodes and send
them a <code>REQ-PING(C)</code> message. Each of these nodes will in turn send
a <code>PING</code> message to *C* and forward the <code>ACK</code> (if
received) to *A*.

<figure>
    <img src="/img/swim_req_ping.svg" />
    <figcaption>SWIM: indirect checks from A to C</figcaption>
</figure>

At the end of the protocol period <code>T</code>, *A* checks if it has received
any direct or indirect response from *C*. If not, it declares *C* as dead in its
local membership list and hands this update off to the dissemination component.

Indirect probing is a great way to avoid any congestion network path between
two nodes and dropped packets on the, which might have caused the direct
<code>PING</code> to have failed in the first place. That's also why the
<code>ACK</code> is relayed back instead of being sent directly to the probing
node. The number of false-positives decreases and the accuracy is improved.

**Note:** the values <code>T</code>, <code>k</code> as well as the timeout
period are parameters of the protocol and can be tuned to suit different needs
and networks.

### Dissemination component

Upon detecting the failure of another group member, the process multicasts this
information to the rest of the cluster as <code>failed(C)</code> message.
A member receiving this message deletes the offending node from its local
membership list. Information about newly joined members or voluntarily
leaving members are disseminated in a similar manner.

<figure>
    <img src="/img/swim_multicast_failure.svg" />
    <figcaption>SWIM: failure of C, detected and multicast by A</figcaption>
</figure>

**Note:** for a new node to join the cluster, it must know the address of
at least one alive node member of the cluster.

## Evaluating *SWIM*

If we sum up what *SWIM* does so far:

 * it splits the failure detection and the dissemination components ;
 * each node of the cluster periodically check the health of a random node of
     its memberlist (either directly or indirectly) ;
 * <code>join()</code> and <code>failed()</code> events are multicast.

So given our evaluation criteria, what can we say about the protocol?

* **Strong Completeness**: as each node randomly selects at each protocol period
    a node to check, **all nodes will eventually be checked** and all faulty-nodes
    will be detected ;
* **Speed of failure detection**: the expected time between a failure of a node
    and its detection by *some* other node is at most
    <code>T' * (1 / (1 - e^Qf))</code>, where <code>Qf</code> is the fraction of
    non-faulty nodes in the cluster and <code>T'</code> is the protocol period
    (in time units, should be at least three times the round-trip estimate) ;
* **False positives**: the probability of direct or indirect probes being lost
    is (gross approximation here, the real equation is a bit larger) <code>(1 - Delivery_%²)*(1 - Delivery_%⁴)^k</code>
    — where <code>Delivery_%</code> is the probability of timely delivery of a
    packet and <code>k</code> is the number of members used to perform indirect
    probing). Notice how <code>k</code> is used as an exponent? It shows that
    **increasing the number of nodes used for indirect probing will rapidly
    decrease the probability of having false positives** ;
* **Accuracy**: as the accuracy is directly correlated to the false positives,
    we can calculate that with 95% of the packets successfully delivered and 3
    nodes used for indirect probing, the protocol will have an accuracy of
    99.9%. Again, this value increases as we increase <code>k</code> ;
* **Network Message Load**: as each node only pings one target each protocol period,
    we moved from an <code>O(N²)</code> to something in <code>O(N)</code>. We
    roughly have two packets (<code>PING</code> and <code>ACK</code>) and an
    optional indirect probe (hence the 4 extra packets per relay node)
    <code>(4*k + 2)*N</code>.

**Important:** the speed of failure detection and the probability of false
positives don't depend on the number of nodes in the cluster (except
asymptotically). Which means that the protocol will scale nicely, even on large
clusters.

## Improving *SWIM*

## *SWIM* in Go: memberlist

 * https://github.com/hashicorp/memberlist

## Links

 * https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf
 * https://www.youtube.com/watch?v=bkmbWsDz8LM
 * https://en.wikipedia.org/wiki/Gossip_protocol
 * https://www.serf.io/docs/internals/gossip.html
 * https://gotocon.com/dl/goto-chicago-2016/slides/WilfriedSchobeiri_BuildingAClusteredServiceInGo.pdf

 * https://www.brianstorti.com/swim/
 * https://asafdav2.github.io/2017/swim-protocol/
 * https://prakhar.me/articles/swim/
